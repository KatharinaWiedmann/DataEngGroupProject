{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team: Data Magicians\n",
    "Members:\n",
    "<p>\n",
    "Arda Putra Ryandika\n",
    "</p>\n",
    "<p>\n",
    "Atthaya Busayaruengrat (Hong)\n",
    "</p>\n",
    "<p>\n",
    "Jingxue (Vera) Cao\n",
    "</p>\n",
    "<p>\n",
    "Katharina Wiedmann  \n",
    "</p>\n",
    "<p>\n",
    "Ying Tung (Debbie) Lau\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "In this project, we are taking three sources of movie review data (csv, tsv, web scraping) and aiming to create weak labelling functions based on the data. The objective of the project is to compare the performance of a machine learning based classifier with that of the combination of weak labelling functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Data\n",
    "\n",
    "We split the work of obtaining data to three group members (Hong, Debbie, Vera) . Hong was responsible for scraping movie reviews from the rotten tomato website, while Debbie and Vera sought for other data formats like tsv and csv to ensure the sufficiency of data sample. \n",
    "\n",
    "The dataset contains two columns: one is the text review posted by people, another is the label 1 or 0. 1 indicates a positive review (fresh), and 0 indicates an non-positive review (not fresh).\n",
    "\n",
    "The final dataset consists of 15000 reviews in total, with 5000 from web scrapping, 5000 from csv file, and 5000 from tsv file. \n",
    "\n",
    "We further sampled 2000 data points for labelling function development (development data set) and made sure the positive and non-positive reviews had the same amounts. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building classifiers \n",
    "\n",
    "Arda was responsible for building a NLP classifier, which will be later applied to compare with the labelling function classifier. This model was done by utilizing tokens generated from two types of reviews and fed as features. This NLP classifier yielded 70% accuracy on the testing set. \n",
    "Arda also implemented the spark on Faculty so that the following labelling functions can be implemented in a spark environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Labelling functions\n",
    "\n",
    "Meanwhile, the rest of the team members worked on generating weak labelling functions based on the findings in data exploration. \n",
    "\n",
    "During data exploration, as implementing spark slowed down the computation process as it partitions the dataset, we chose to use pandas to notice any patterns and differences in positive and non-positive reviews. We faced a few challenges in this stage. For example, we built a lemmatization function on the word count dataframe to avoid classifier bias caused by word inflections. However, many words were converted into completely different words incorrectly. Due to the mis-correction on words, we decided not to use lemmatization. \n",
    "\n",
    "As for building the labelling functions, Katie looked for stop words in the review first and counted the word occurrences in positive reviews and negative reviews. By identifying the difference in the word occurrences, we built our labelling functions to separate positive and non-positive reviews based on exclusive words. \n",
    "\n",
    "We also looked for capital letters mentioned in each type of reviews, but since the capital letters were irrelevant to emotions and most didn’t make sense for understanding, we decided not to create a labelling function based on it.\n",
    "\n",
    "As we noticed keywords like “too” and “far” occurred more often in a specific type of reviews, so other labelling functions were created based on the keywords. \n",
    "Similarly, exclamation mark and question mark also appeared more often in one type of reviews, so we created labelling functions based on them as well.\n",
    "\n",
    "After finalising the labeling functions, Arda built a classifier to combine the labeling functions together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub\n",
    "Every time we made a change, we used terminal in Faculty to push the changes to our group repository. \n",
    "After committing the change, we used “git status” to double check the state of repository. Using “git diff” also enables us to see all the changes in repository.\n",
    "\n",
    "Link to GitHub repository:\n",
    "https://github.com/KatharinaWiedmann/DataEngGroupProject.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIRA board \n",
    "We used JIRA to manage the progress of our project and record our meeting topics. The brief of meetings is shown as below:\n",
    "<p>\n",
    "First meeting:\n",
    "•\tRotten Tomatoes labelling functions - good movie or not (positive/ negative rating). \n",
    "•\tTwitter data: Labelling whether someone is a Boris Johnson supporter or not. \n",
    "•\tYouTube comments: positive or negative comment \n",
    "•\tPromotion emails - is an email a spam email or a genuine email. \n",
    "</p>\n",
    "<p>\n",
    "Second meeting:\n",
    "•\tLabelling functions (Vera, Debbie, Katie) \n",
    "•\tClassifier (Arda) \n",
    "•\tWeb Scraping (Hong) \n",
    "•\tGitHub (Katie)\n",
    "</p>\n",
    "<p>\n",
    "Third meeting:\n",
    "•\tRewriting labelling functions (SparkLFApplier - done together)\n",
    "•\tCombining labelling functions (Hong & Katie)\n",
    "•\tAnalyzing summary/ results labelling functions (Hong & Katie)\n",
    "•\tPlug in classifier (Arda)\n",
    "•\tCompare results between using labelling functions and not labelling functions (Arda)\n",
    "•\tIterate on Mark-up/ write-down (Vera)\n",
    "•\tJIRA cleanup & additional notes (Katie)\n",
    "•\tGithub reminder - don't forget to push/ pull (everyone)\n",
    "•\tMake sensitivity analysis work (Debbie) \n",
    "•\tnt function (Hong)\n",
    "</p>\n",
    "\n",
    "Further details can be found at:\n",
    "http://csjira2.cs.ucl.ac.uk:8080/secure/RapidBoard.jspa?rapidView=316&view=detail&selectedIssue=DED-16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pandas==0.24.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install networkx==2.3.0\n",
    "# run once and then need to restart the kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install networkx==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#needs to show version 2.3\n",
    "import networkx as nx\n",
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda/envs/Python3\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  networkx                                         2.3-py_0 --> 2.4-py_0\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda upgrade --all -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda/envs/Python3\n",
      "\n",
      "  added / updated specs:\n",
      "    - snorkel==0.9.0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.11.28         |           py36_0         149 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         149 KB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2019.11.28-hecc5488_0\n",
      "  certifi                                         pkgs/main --> conda-forge\n",
      "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_4 --> conda-forge::openssl-1.1.1d-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "certifi-2019.11.28   | 149 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install snorkel==0.9.0 -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/faculty/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier,LabelModel\n",
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from itertools import repeat\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from csv import writer\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import json\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[4] appName=pyspark-shell>\n",
      "<pyspark.sql.session.SparkSession object at 0x7faa8ace8f60>\n"
     ]
    }
   ],
   "source": [
    "#Spark \n",
    "\n",
    "# Spark Environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "import pyspark\n",
    "\n",
    "number_cores = 4\n",
    "memory_gb = 16\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "print(sc)\n",
    "\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark) \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Browse all from DVD releases page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = 'https://www.rottentomatoes.com/api/private/v2.0/browse?maxTomato=100&services=amazon%3Bhbo_go%3Bitunes%3Bnetflix_iw%3Bvudu%3Bamazon_prime%3Bfandango_now&certified&sortBy=release&type=dvd-streaming-all&page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movie url\n",
    "movie_url = []\n",
    "start_page = 1 ; end_page = 1\n",
    "while start_page <= end_page:\n",
    "#     time.sleep(7)\n",
    "    url = main + str(start_page)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code !=200:\n",
    "        print('Request error')\n",
    "        break\n",
    "    file = json.loads(response.text)\n",
    "    for i in file['results']:\n",
    "        movie_url = movie_url + [i['url']]\n",
    "    start_page +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples for the url:\n",
      "\n",
      "/m/frozen_ii\n",
      "/m/playmobil_the_movie\n",
      "/m/charlies_angels_2019\n",
      "\n",
      "Number of movies in list: 32\n"
     ]
    }
   ],
   "source": [
    "print('Examples for the url:\\n')\n",
    "for i in range(3):\n",
    "    print(movie_url[i])\n",
    "print('\\nNumber of movies in list: {}'.format(len(movie_url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into lists of 50 movies to do the scraping\n",
    "movie_url_split = [movie_url[i:i+50] for i in range(0,600,50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_url_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2d0f2816c27e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Request error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Get labels from each review (fresh vs. rotten)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/html/parser.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/html/parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"</\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<!--\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/html/parser.py\u001b[0m in \u001b[0;36mparse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgtpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_cdata_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgtpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/builder/_htmlparser.py\u001b[0m in \u001b[0;36mhandle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malready_closed_empty_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_endtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mhandle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \"\"\"\n\u001b[1;32m    650\u001b[0m         \u001b[0;31m#print \"End tag: \" + name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popToTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36mendData\u001b[0;34m(self, containerClass)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontainerClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_was_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/Python3/lib/python3.6/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_OUTPUT_ENCODING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get reviews from the web\n",
    "reviews = []\n",
    "titles = []\n",
    "ratings = []\n",
    "for split in movie_url_split: # Loop through each split\n",
    "#     time.sleep(7)\n",
    "    for title in split: # Loop through each movie title\n",
    "        url = 'https://www.rottentomatoes.com'+title\n",
    "#         time.sleep(7)\n",
    "        response = requests.get(url)\n",
    "        # Check the request status code\n",
    "        if response.status_code != 200:\n",
    "            print('Request error')\n",
    "            break\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Get labels from each review (fresh vs. rotten)\n",
    "        fresh_rotten = soup.find_all(class_=\"review_quote\")\n",
    "        \n",
    "        # Get movie title\n",
    "        title = soup.find(class_=\"mop-ratings-wrap__title mop-ratings-wrap__title--top\").getText()\n",
    "        \n",
    "        # Get reviews\n",
    "        review = soup.find_all('blockquote')\n",
    "        for i in review:\n",
    "            j = str(i.contents[1])\n",
    "            j = j.replace(\"<p>\\n                    \\n                        \",\"\")\n",
    "            j = j.replace(\"\\n                    \\n                </p>\",\"\")\n",
    "            reviews = reviews + [j]\n",
    "            titles = titles + [title]\n",
    "        \n",
    "        # Identify labels\n",
    "        for i in fresh_rotten:\n",
    "            temp = str(i.findChildren()[2])\n",
    "            if re.search('rotten',temp):\n",
    "                ratings = ratings + ['rotten']\n",
    "            else:\n",
    "                ratings = ratings + ['fresh']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame to store the scraped data\n",
    "df = pd.DataFrame([titles,reviews,ratings],index = ['title','review','rating']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data (drop duplicates, check missing values etc.)\n",
    "df = df.drop_duplicates()\n",
    "df = df.replace([None],np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV files\n",
    "# df.to_csv('web_scraping_rotten_tomatoes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Preparing TSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read TSV file\n",
    "tsv_reviews = pd.read_csv('/project/reviews.tsv', sep='\\t', header=0, encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract review and fresh columns\n",
    "tsv_reviews = pd.DataFrame(tsv_reviews, columns = ['review', 'fresh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NaN rows in reviews\n",
    "index_name = tsv_reviews[(tsv_reviews['review'].isnull())].index\n",
    "tsv_reviews.drop(index_name, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsv_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename fresh as 1 and rotten as 0\n",
    "tsv_reviews['fresh'].replace({'fresh':'1', 'rotten':'0'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns\n",
    "tsv_reviews.rename(columns={'fresh':'Freshness','review':'Review'},inplace=True)\n",
    "tsv_reviews = tsv_reviews.sample(5000)\n",
    "#take 5000\n",
    "tsv_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading & Preparing CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "csv_reviews= pd.read_csv('/project/rotten_tomatoes_reviews.csv')\n",
    "csv_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swap Freshness and Review \n",
    "columns_titles = [\"Review\",\"Freshness\"]\n",
    "csv_reviews=csv_reviews.reindex(columns=columns_titles)\n",
    "csv_reviews = csv_reviews.sample(5000)\n",
    "\n",
    "csv_reviews.head()\n",
    "csv_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping & Preparing scrapped data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Web Scraping Data\n",
    "web_scraping_reviews= pd.read_csv('/project/web_scraping_rotten_tomatoes.csv')\n",
    "web_scraping_reviews.head()\n",
    "\n",
    "web_scraping_reviews = web_scraping_reviews.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename fresh as 1 and rotten as 0\n",
    "web_scraping_reviews['rating'].replace({'fresh':'1', 'rotten':'0'}, inplace = True)\n",
    "\n",
    "#Rename Rating to Review \n",
    "web_scraping_reviews.rename(columns={'rating':'Freshness', 'review':'Review'},inplace=True)\n",
    "web_scraping_reviews.head()\n",
    "\n",
    "# Extract Review and Freshness columns\n",
    "web_scraping_reviews= pd.DataFrame(web_scraping_reviews, columns = ['Review', 'Freshness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scraping_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining all the data together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_reviews.info()\n",
    "# tsv_reviews.info()\n",
    "# web_scraping_reviews.info()\n",
    "\n",
    "\n",
    "\n",
    "# Concat two files into all_reviews\n",
    "all_reviews=pd.concat([csv_reviews, tsv_reviews,web_scraping_reviews],axis=0, sort=False)\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into test and training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reviews['Freshness'] = all_reviews['Freshness'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(all_reviews,test_size=0.2,stratify = all_reviews['Freshness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid off training labels \n",
    "train = train.drop('Freshness', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Freshness'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From labelled test set, extract a sample to find out about which labelling functions could be written\n",
    "#Not sure how big the development split_ can be --> take sample of 1000 data points \n",
    "\n",
    "development_split = test.sample(2000,random_state=42)\n",
    "development_split.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2000.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1000 = pd.merge(test,test2000,how='left',on = 'Review')\n",
    "# test1000 = test1000[test1000.Freshness_y.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1000.drop('Freshness_y',axis=1, inplace=True)\n",
    "# test1000.columns = ['Review','Freshness']\n",
    "# test1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1000.to_csv('1000_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#For finding labelling functions: \n",
    "development_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split.to_csv('development_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development_split = pd.read_csv('development_split.csv',index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Might have to get rid off index?\n",
    "\n",
    "development_split[development_split['Freshness'] !=1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split['Freshness'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split = pd.read_csv('development_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split_fresh = development_split[development_split['Freshness'] == 1]\n",
    "development_split_rotten = development_split[development_split['Freshness'] == 0]\n",
    "development_split_fresh.head()\n",
    "development_split_rotten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fresh reviews \n",
    "development_split_fresh.info()\n",
    "# rotten reviews \n",
    "development_split_rotten.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation \n",
    "def remove_punctuation(dataframe):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in dataframe.Review.str.lower():\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation from fresh reviews & turn into Series\n",
    "split_fresh= pd.Series(remove_punctuation(development_split_fresh))\n",
    "split_fresh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWordList = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\",\\\n",
    "                \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\\\n",
    "                \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\",\\\n",
    "                \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\",\\\n",
    "                \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\",\\\n",
    "                \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\",\\\n",
    "                \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\",\\\n",
    "                \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\",\\\n",
    "                \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stopwords from fresh\n",
    "\n",
    "replacements = dict(zip((fr'\\b{word}\\b' for word in stopWordList), repeat(\"\")))\n",
    "split_fresh.replace(replacements, regex=True, inplace=True)\n",
    "split_fresh.replace({r' +': ' ', r' +\\.': '.'}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization and Count again\n",
    "def stem_recount(df):\n",
    "    import pandas as pd\n",
    "    # Lemmatization\n",
    "    from nltk import LancasterStemmer\n",
    "    st = LancasterStemmer()\n",
    "    newdf = df.copy()\n",
    "    for i in range(0,len(df)):\n",
    "        newdf.iloc[i,0] = st.stem(str(df.iloc[i,0])) \n",
    "        # Plz make sure the word column is the first column in df when using this function\n",
    "    \n",
    "    # Recount\n",
    "    duplicate = newdf[newdf.duplicated(['index'])]\n",
    "    # Plz make sure the 'index' is the column name consisting of words\n",
    "    for i in range(0,len(newdf)):\n",
    "        if i >= len(duplicate):\n",
    "            break\n",
    "        if newdf.iloc[i,0] == duplicate.iloc[i,0]:\n",
    "            newdf.iloc[i,1] = newdf.iloc[i,1] + duplicate.iloc[i,1]\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_fresh = split_fresh.str.split(expand=True).stack().value_counts()\n",
    "common_words_fresh_df = pd.DataFrame(common_words_fresh)\n",
    "common_words_fresh_df = common_words_fresh_df.rename({0:'Occurence good review'}, axis='columns')\n",
    "new_common_words_fresh_df = common_words_fresh_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_recount(new_common_words_fresh_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get most common words in positive reviews \n",
    "top_common_words_fresh = common_words_fresh_df[common_words_fresh_df['Occurence good review'] >=4]\n",
    "top_common_words_fresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** EXPLAIN WHY WE DONT USE LEMMATIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b> Reason for not using Lemmatization </b>\n",
    "\n",
    "<p>\n",
    "    Before counting the occurance of words in the movie review, we noticed that inflections in words may result in different occurances and thus generating bias during counting. For example, \"enjoy\" and \"enjoyed\" share the same root but would be counted separately if not using Lemmatization.\n",
    "    </p> \n",
    "    \n",
    "<p>\n",
    "    The function \"stem_recount\" takes the root of a word and recounts the occurences. However, it posed a disadvantage of mis-normalizing words into other completely different words. For example, \"movie\" was identified as \"movy\", and \"like\" was identified as \"lik\". We thought this disadvantage exceeds the benefits of correcting word inflection, so we decided to not implement it.\n",
    "    </p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation from rotten & turn into Series\n",
    "split_rotten= pd.Series(remove_punctuation(development_split_rotten))\n",
    "split_rotten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords from negative reviews\n",
    "replacements = dict(zip((fr'\\b{word}\\b' for word in stopWordList), repeat(\"\")))\n",
    "split_rotten.replace(replacements, regex=True, inplace=True)\n",
    "split_rotten.replace({r' +': ' ', r' +\\.': '.'}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get most common words in negative reviews \n",
    "\n",
    "common_words_rotten = split_rotten.str.split(expand=True).stack().value_counts()\n",
    "common_words_rotten_df = pd.DataFrame(common_words_rotten)\n",
    "common_words_rotten_df = common_words_rotten_df.rename({0:'Occurence bad review'}, axis='columns')\n",
    "top_common_words_rotten = common_words_rotten_df[common_words_rotten_df['Occurence bad review'] >=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_common_words_rotten.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of good and bad reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We want to find out which of the words in the good list only appear in the good movies (and not in the bad movies), vice versa and base labeling functions on these findings. We first ened to prepare the data accordingly, before we can write the labelling functions.\n",
    "    <div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fresh_words_exclusive = top_common_words_fresh.merge(top_common_words_rotten, indicator='i', how='outer', left_index=True,\\\n",
    "                                                         right_index=True).query('i == \"left_only\"').drop('i', 1)\n",
    "\n",
    "top_rotten_words_exclusive = top_common_words_rotten.merge(top_common_words_fresh, indicator='i', how='outer', left_index=True,\\\n",
    "                                                           right_index=True).query('i == \"left_only\"').drop('i', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_fresh_words_exclusive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rotten_words_exclusive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only positive words \n",
    "top_fresh_words_exclusive_list = top_fresh_words_exclusive['Occurence good review'].index.tolist()\n",
    "top_fresh_words_exclusive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out the ones that seem to make sense: \n",
    "top_fresh_words_exclusive = ['absolutely',\n",
    " 'addition',\n",
    " 'adventure',\n",
    " 'affectionate',\n",
    " 'amazing',\n",
    " 'ambition',\n",
    " 'art',\n",
    " 'artist',\n",
    " 'arts',\n",
    " 'atmosphere',\n",
    " 'attractive',\n",
    " 'awards',\n",
    " 'balance',\n",
    " 'beautiful',\n",
    " 'beautifully',\n",
    " 'beauty',\n",
    " 'bond',\n",
    " 'bright',\n",
    " 'brilliant',\n",
    " 'captivating',\n",
    " 'captures',\n",
    " 'celebration',\n",
    " 'charm',\n",
    " 'charming',\n",
    " 'christmas',\n",
    " 'classic',\n",
    " 'clever',\n",
    " 'committed',\n",
    " 'consistently',\n",
    " 'contemporary',\n",
    " 'conventional',\n",
    " 'convincingly',\n",
    " 'creates',\n",
    " 'creating',\n",
    " 'crowdpleaser',\n",
    " 'cult',\n",
    " 'decade',\n",
    " 'decades',\n",
    " 'deep',\n",
    " 'deeper',\n",
    " 'deeply',\n",
    " 'definitely',\n",
    " 'delightful',\n",
    " 'delightfully',\n",
    " 'depth',\n",
    " 'deserves',\n",
    " 'design',\n",
    " 'details',\n",
    " 'different',\n",
    " 'diverse',\n",
    " 'dramatic',\n",
    " 'early',\n",
    " 'elegant',\n",
    " 'emotionally',\n",
    " 'engaging',\n",
    " 'enjoyable',\n",
    " 'enjoyed',\n",
    " 'equal',\n",
    " 'especially',\n",
    " 'exploration',\n",
    " 'extraordinary',\n",
    " 'extremely',\n",
    " 'familiar',\n",
    " 'famous',\n",
    " 'fan',\n",
    " 'fantastic',\n",
    " 'fantasy',\n",
    " 'fascinating',\n",
    " 'felt',\n",
    " 'filled',\n",
    " 'finest',\n",
    " 'frank',\n",
    " 'fresh',\n",
    " 'friends',\n",
    " 'friendship',\n",
    " 'gags',\n",
    " 'gorgeous',\n",
    " 'grand',\n",
    " 'happy',\n",
    " 'heart',\n",
    " 'hilarious',\n",
    " 'honest',\n",
    " 'hope',\n",
    " 'huge',\n",
    " 'impact',\n",
    " 'insightful',\n",
    " 'inspiring',\n",
    " 'intelligent',\n",
    " 'intense',\n",
    " 'intrigue',\n",
    " 'joy',\n",
    " 'laugh',\n",
    " 'loved',\n",
    " 'mature',\n",
    " 'mind',\n",
    " 'mystery',\n",
    " 'nostalgia',\n",
    " 'novel',\n",
    " 'opening',\n",
    " 'passion',\n",
    " 'perfect',\n",
    " 'performers',\n",
    " 'personal',\n",
    " 'pleasure',\n",
    " 'poignant',\n",
    " 'power',\n",
    " 'powerful',\n",
    " 'precisely',\n",
    " 'profound',\n",
    " 'project',\n",
    " 'proves',\n",
    " 'provide',\n",
    " 'provocative',\n",
    " 'psychological',\n",
    " 'quality',\n",
    " 'remarkable',\n",
    " 'reveals',\n",
    " 'rich',\n",
    " 'riveting',\n",
    " 'satisfying',\n",
    " 'sharp',\n",
    " 'simple',\n",
    " 'smart',\n",
    " 'smile',\n",
    " 'stunning',\n",
    " 'succeeds',\n",
    " 'supernatural',\n",
    " 'surprise',\n",
    " 'surprises',\n",
    " 'surprising',\n",
    " 'surprisingly',\n",
    " 'sweet',\n",
    " 'talents',\n",
    " 'thoughtful',\n",
    " 'thrills',\n",
    " 'touch',\n",
    " 'touching',\n",
    " 'tragedy',\n",
    " 'tragic',\n",
    " 'tribute',\n",
    " 'unique',\n",
    " 'universal',\n",
    " 'warm',\n",
    " 'watchable',\n",
    " 'welcome',\n",
    " 'wit',\n",
    " 'witty',\n",
    " 'wonderful',\n",
    " 'worthwhile',\n",
    " 'worthy']\n",
    "\n",
    "top_fresh_words_exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only negative words \n",
    "top_rotten_words_exclusive_list = top_rotten_words_exclusive['Occurence bad review'].index.tolist()\n",
    "top_rotten_words_exclusive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#take out the ones that seem to make sense: \n",
    "top_rotten_words_exclusive = [\n",
    " 'attempt',\n",
    " 'awkward',\n",
    " 'barely',\n",
    " 'basically',\n",
    " 'bizarre',\n",
    " 'bland',\n",
    " 'boring',\n",
    " 'clumsy',\n",
    " 'comedic',\n",
    " 'disappointing',\n",
    " 'disappointingly',\n",
    " 'disappointment',\n",
    " 'disaster',\n",
    " 'dull',\n",
    " 'effort',\n",
    " 'failed',\n",
    " 'fails',\n",
    " 'generic',\n",
    " 'irritating',\n",
    " 'lacking',\n",
    " 'manic',\n",
    " 'missing',\n",
    " 'nobody',\n",
    " 'noir',\n",
    " 'none',\n",
    " 'painfully',\n",
    " 'pointless',\n",
    " 'poorly',\n",
    " 'problem',\n",
    " 'shallow',\n",
    " 'shame',\n",
    " 'sloppy',\n",
    " 'slow',\n",
    " 'suffers',\n",
    " 'superficial',\n",
    " 'try',\n",
    " 'unfortunately',\n",
    " 'unfunny',\n",
    " 'worst']\n",
    "top_rotten_words_exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word Occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A. Good / bad exclusive words occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "\n",
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext \n",
    "import pandas as pd \n",
    "sqlc=SQLContext(sc) \n",
    "df=pd.read_csv('/project/development_split.csv',index_col = 'Unnamed: 0')\n",
    "df_with_punctuation = df.copy()\n",
    "df['Review'] = remove_punctuation(df)\n",
    "development_split=sqlc.createDataFrame(df)\n",
    "development_split_with_punctuation=sqlc.createDataFrame(df_with_punctuation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "development_split.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development_split = pd.read_csv('/project/development_split.csv')\n",
    "ABSTAIN = -1\n",
    "NOTFRESH = 0\n",
    "FRESH = 1\n",
    "\n",
    "@labeling_function()\n",
    "def fresh(x):\n",
    "    for word in top_fresh_words_exclusive:\n",
    "        word = \" \" +word+\" \"\n",
    "        if word in str(x).lower():\n",
    "            return FRESH\n",
    "    return ABSTAIN\n",
    "#return FRESH if \"best\" in x.str.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def rotten(x):\n",
    "    for word in top_rotten_words_exclusive:\n",
    "        word = \" \" +word+\" \"\n",
    "        if word in str(x).lower():\n",
    "            return NOTFRESH\n",
    "    return ABSTAIN\n",
    "#return NOTFRESH if \"best\" in x.str.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [fresh]\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_fresh = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"fresh coverage:{:.1%}\".format(coverage_fresh[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [rotten]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_rotten = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"rotten coverage:{:.1%}\".format(coverage_rotten[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Word 'too' occurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_fresh_df[common_words_fresh_df.index == 'too']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_words_rotten_df[common_words_rotten_df.index == 'too']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def keyword_too(x):\n",
    "    return NOTFRESH if 'too' in str(x).lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [keyword_too]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_keyword_too = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"keyword too coverage:{:.1%}\".format(coverage_keyword_too[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Word 'far' occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_fresh_df[common_words_fresh_df.index == 'far']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_rotten_df[common_words_rotten_df.index == 'far']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def keyword_far(x):\n",
    "    return FRESH if 'far' in str(x).lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [keyword_far]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_keyword_far = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"keyword far coverage:{:.1%}\".format(coverage_keyword_far[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. \"n't\" words occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration on the n't\n",
    "# Word occurancy that with punctuation with it\n",
    "\n",
    "# Word occurrences dataframe for fresh reviews\n",
    "development_split_fresh_1 = split_fresh.str.split(expand=True).stack().value_counts()\n",
    "development_split_fresh_df = pd.DataFrame(development_split_fresh_1).reset_index()\n",
    "\n",
    "# Words occurrences dataframe for rotten reviews\n",
    "development_split_rotten_1 = split_rotten.str.split(expand=True).stack().value_counts()\n",
    "development_split_rotten_df = pd.DataFrame(development_split_rotten_1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "\n",
    "def t(x):\n",
    "    if re.search(\"'t\",str(x).lower()):\n",
    "        return NOTFRESH\n",
    "    return ABSTAIN\n",
    "#return FRESH if \"best\" in x.str.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [t]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_t = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"keyword far coverage:{:.1%}\".format(coverage_t[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Occurenes of good & bad words from external list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We also want to look at an imported list of postive and negative words and see whether we can base the labelling functions on them.\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing good and bad words & preparing for labelling function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POSITIVE WORDS \n",
    "#positive words from --> DON't DELETE! NEED TO CITE PROPERLY http://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "positive_word = pd.read_csv('/project/positive_words.csv')\n",
    "\n",
    "#sample 500 words \n",
    "positive_word = positive_word.sample(500)\n",
    "\n",
    "#convert it into a list \n",
    "positive_word= positive_word['a+'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "#NEGATIVE WORDS \n",
    "#negative words from --> HONG?? \n",
    "negative_word = pd.read_csv('/project/negative_words.csv')\n",
    "\n",
    "#sample 500 words \n",
    "negative_word = negative_word.sample(500)\n",
    "\n",
    "#convert it into a list \n",
    "negative_word= negative_word['2-faces'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def negative(x): \n",
    "    for word in negative_word:\n",
    "        word = \" \" + word + \" \"\n",
    "        if word in str(x).lower():\n",
    "            return NOTFRESH \n",
    "    return ABSTAIN \n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def positive(x):\n",
    "    for word in positive_word:\n",
    "        word = \" \" + word + \" \"\n",
    "        if word in str(x).lower():\n",
    "            return FRESH \n",
    "    return ABSTAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [negative]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_negative = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"negative words coverage:{:.1%}\".format(coverage_negative[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [positive]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_positive = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"positive words coverage:{:.1%}\".format(coverage_positive[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Punctuation occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn review column into Series\n",
    "development_split_fresh_series = pd.Series(development_split_fresh.Review)\n",
    "development_split_rotten_series = pd.Series(development_split_rotten.Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive reviews\n",
    "# Split reviews into word\n",
    "fresh_split = pd.Series(development_split_fresh_series.str.split(expand=True).stack())\n",
    "fresh_words = [i for i in fresh_split]\n",
    "\n",
    "# Split words into characters\n",
    "def split_str():\n",
    "    return [list(ch) for ch in fresh_words]\n",
    "fresh_split_words = pd.Series(split_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative reviews\n",
    "# Split reviews into word\n",
    "rotten_split = pd.Series(development_split_rotten_series.str.split(expand=True).stack())\n",
    "rotten_words = [i for i in rotten_split]\n",
    "\n",
    "# Split words into characters\n",
    "def split_str():\n",
    "    return [list(ch) for ch in rotten_words]\n",
    "rotten_split_words = pd.Series(split_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into a flattened list\n",
    "fresh_flattened_list = [y for x in fresh_split_words for y in x]\n",
    "rotten_flattened_list = [y for x in rotten_split_words for y in x]\n",
    "\n",
    "# Count the occurancy of each character\n",
    "# Positive reviews\n",
    "fresh_split_characters = pd.Series(fresh_flattened_list).value_counts()\n",
    "fresh_split_characters = pd.DataFrame(fresh_split_characters).reset_index()\n",
    "\n",
    "# Negative reviews\n",
    "rotten_split_characters = pd.Series(rotten_flattened_list).value_counts()\n",
    "rotten_split_characters = pd.DataFrame(rotten_split_characters).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Question mark occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the # of occurance of '?' in fresh reviews\n",
    "fresh_split_characters[fresh_split_characters['index'] == '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the # of occurance of '?' in rotten reviews\n",
    "rotten_split_characters[rotten_split_characters['index'] == '?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_question_mark = []\n",
    "for review in development_split_rotten.Review:\n",
    "    if '?' in review:\n",
    "        list_with_question_mark.append(review)\n",
    "        \n",
    "print (list_with_question_mark[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def question_mark(x):\n",
    "    return NOTFRESH if '?' in str(x).lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lfs = [question_mark]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split_with_punctuation.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_question_mark = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"question mark coverage:{:.1%}\".format(coverage_question_mark[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Exclamation mark occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the # of occurance of '!' in fresh reviews\n",
    "fresh_split_characters[fresh_split_characters['index'] == '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the # of occurance of '!' in rotten reviews\n",
    "rotten_split_characters[rotten_split_characters['index'] == '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def exclamation_mark(x):\n",
    "    return FRESH if '!' in str(x).lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lfs = [exclamation_mark]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split_with_punctuation.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_exclamation_mark = (sample_L != ABSTAIN).mean(axis=0)\n",
    "print(\"exclamation mark coverage:{:.1%}\".format(coverage_exclamation_mark[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combining labelling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Next, we want to combine all the labelling functions into one and apply them to the training set. However, as the labelling functions around the punctuation have very low coverages, we decided not to include these.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [fresh,\n",
    "       rotten,\n",
    "       keyword_too,\n",
    "       keyword_far,\n",
    "       t,\n",
    "       negative,\n",
    "       positive]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "sample_L = applier.apply(development_split.rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prepared = train.copy()\n",
    "\n",
    "train_prepared\n",
    "train_prepared['Review'] = remove_punctuation(train_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "LFAnalysis(L=sample_L, lfs = lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [fresh,\n",
    "       rotten,\n",
    "       keyword_too,\n",
    "       keyword_far,\n",
    "       t,\n",
    "       negative,\n",
    "       positive]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "\n",
    "L_train=sqlc.createDataFrame(train_prepared)\n",
    "\n",
    "#is this next line correct?\n",
    "L_test = sqlc.createDataFrame(test)\n",
    "\n",
    "# type(L_train)\n",
    "L_train = applier.apply(L_train.rdd)\n",
    "\n",
    "#is this next line correct?\n",
    "L_test = applier.apply(L_test.rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.copy()\n",
    "train2['predicted_train'] = preds_train\n",
    "train2.to_csv('12000_predicted_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needs to show version 2.3\n",
    "import networkx as nx\n",
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling according to weights \n",
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to be checked \n",
    "L_test = L_test\n",
    "Y_test = test['Freshness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "\n",
    "#pick a reasonable threshold \n",
    "#Using a lower threshold than other examples as this could be a good indicator for determining official sources vs general negative sentiment\n",
    "@labeling_function()\n",
    "def textblob_polarity(x):\n",
    "    return NOTFRESH if x.polarity > 0.8 else ABSTAIN\n",
    "\n",
    "#do the same for the subjectivity scores. \n",
    "#Using a higher threshold than other examples as this could be a good indicator for determining official sources\n",
    "#This will run faster than the last cell, since we memoized the Preprocessor outputs.\n",
    "@labeling_function()\n",
    "def textblob_subjectivity(x):\n",
    "    return FRESH if x.subjectivity >= 0.6 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(text):\n",
    "    x = {}\n",
    "    x[\"polarity\"] = TextBlob(text).sentiment.polarity\n",
    "    x[\"subjectivity\"] = TextBlob(text).sentiment.subjectivity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @labeling_function(pre=[textblob_sentiment])\n",
    "# def textblob_polarity(x):\n",
    "#     x = getSentiment(x.text)\n",
    "#     return FRESH if x.polarity > 0.8 else ABSTAIN\n",
    "\n",
    "# @labeling_function(pre=[textblob_sentiment])\n",
    "# def textblob_subjectivity(x):\n",
    "#     x = getSentiment(x.text)\n",
    "#     return FRESH if x.subjectivity >= 0.5 else ABSTAIN\n",
    "\n",
    "\n",
    "\n",
    "#pick a reasonable threshold \n",
    "#Using a lower threshold than other examples as this could be a good indicator for determining official sources vs general negative sentiment\n",
    "#@labeling_function(pre=[textblob_sentiment])\n",
    "@labeling_function()\n",
    "def textblob_polarity(x):\n",
    "    x = getSentiment(x.text)\n",
    "    return FRESH if x[\"polarity\"] > 0.8 else ABSTAIN\n",
    "\n",
    "#do the same for the subjectivity scores. \n",
    "#Using a higher threshold than other examples as this could be a good indicator for determining official sources\n",
    "#This will run faster than the last cell, since we memoized the Preprocessor outputs.\n",
    "#@labeling_function(pre=[textblob_sentiment])\n",
    "@labeling_function()\n",
    "def textblob_subjectivity(x):\n",
    "    x = getSentiment(x.text)\n",
    "    return FRESH if x[\"subjectivity\"] >= 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = en_tweets_df\n",
    "lfs = [\n",
    "       textblob_polarity,\n",
    "       textblob_subjectivity\n",
    "      ]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "\n",
    "# L_train = applier.apply(L_train.rdd)\n",
    "\n",
    "\n",
    "development_split = sqlContext.read.csv('/project/development_split.csv', header=True)\n",
    "\n",
    "sample_L = applier.apply(development_split.rdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L_sample, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Spark Environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "import pyspark\n",
    "\n",
    "number_cores = 4\n",
    "memory_gb = 16\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "print(sc)\n",
    "\n",
    "# get the context\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "print(spark) \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "!pip install langid\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preproc as pp\n",
    "\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Read the data (Spark)\n",
    "review_df = sqlContext.read.csv('/project/development_split.csv', header=True)\n",
    "\n",
    "# Rename Column\n",
    "review_df = review_df.withColumnRenamed('Review','text')\n",
    "review_df = review_df.withColumnRenamed('Freshness','label')\n",
    "review_df = review_df.withColumnRenamed('_c0','index')\n",
    "\n",
    "# Change data type to Integer\n",
    "review_df = review_df.withColumn(\"label\", review_df[\"label\"].cast(IntegerType()))\n",
    "\n",
    "# Show df information\n",
    "review_df.show()\n",
    "review_df.printSchema()\n",
    "review_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "review_df = review_df.withColumn(\"text\", remove_stops_udf(review_df[\"text\"]))\n",
    "\n",
    "# remove other non essential words\n",
    "review_df = review_df.withColumn(\"text\", remove_features_udf(review_df[\"text\"]))\n",
    "\n",
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "review_df = review_df.withColumn(\"text\", tag_and_remove_udf(review_df[\"text\"]))\n",
    "\n",
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "review_df = review_df.withColumn(\"text\", lemmatize_udf(review_df[\"text\"]))\n",
    "\n",
    "review_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Training and Test data\n",
    "training_df = review_df\n",
    "test_df = sqlContext.read.csv('/project/1000_labels.csv', header=True)\n",
    "\n",
    "# Rename Column\n",
    "test_df = test_df.withColumnRenamed('Review','text')\n",
    "test_df = test_df.withColumnRenamed('Freshness','label')\n",
    "test_df = test_df.withColumnRenamed('_c0','index')\n",
    "\n",
    "# Change data type to Integer\n",
    "test_df = test_df.withColumn(\"label\", test_df[\"label\"].cast(IntegerType()))\n",
    "\n",
    "\n",
    "test_df.show()\n",
    "test_df.printSchema()\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol='words', outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")\n",
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(training_df)\n",
    "\n",
    "result = cvModel.transform(test_df)\n",
    "prediction_df = result.select(\"text\", \"label\", \"prediction\")\n",
    "prediction_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluate the Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(result, {evaluator.metricName: \"accuracy\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
